---
title: "State-level choropleth map"
author: "Manish"
date: "5/28/2020"
output: html_document
---
## 1.1. Mapping Case Count
Recall the two points that we discussed. The first was that we need a high-level notion of what do we want our map to communicate (or some idea about what do we want to explore). And the second point was that given a certain data - spatial configuration and attribute values - and our objectives, how can we narrow down to one or more optimum maps? 

We want to create a map showing spatial distribution of total covid cases at state level. Recall that this is a cumulative total upto the latest date available in the data. We will not try to answer the second question precisely (I am not sure if a precise answer exists or even desirable), but we will try to develop certain princples that can help us narrow down to a small number of options.

Load the state-level covid count data that we saved as GeoPackage. If you are a regular GIS user you might like to use GeoPackage more often and get rid of the highly restricted shapefile, if you still use it. 
```{r, echo=FALSE, result='hide', warning=FALSE, message=FALSE}
invisible(lapply(c('sp', 'sf', 'classInt', 'RColorBrewer', 'ggplot2', 'leaflet', 'tidycensus', 'data.table', 'dplyr', 'tigris'), require, character.only=TRUE))
state.total = st_read('/Users/manishve/Desktop/covid.gpkg', quiet=TRUE)
state.total = state.total[c('GEOID', 'cases', 'deaths', 'state_abbr')]
```
We will work with the number of COVID cases in each state. Let us develop an idea of how the attribute values are distributed. Let the Jenks curve (also known as Lorenz curve) and Fisher diagram, we discussed, provide a loose framework to summarize the data. Remember that we want to divide our data into a small number of clusters that capture the essence of what we are interested in and also faithfully represent the data.

```{r Fig1, echo=TRUE, fig.height=3, fig.width= 7}
cases.cum = ecdf(state.total$cases)
print(quantile(cases.cum))
par(mfrow = c(1,3))
plot(cases.cum, main = 'Cumulative', xlab = 'Number of cases')
hist(state.total$cases, 10, main = 'Histogram', xlab = 'Number of Cases')
hist(log10(state.total$cases), 10, main = 'Histogram of LogCases', xlab = 'Log of no of cases')
```
Let us look at the outputs from the cell above. Notice that the number of cases range from 468 to 355037, so the maximum is about 750 times more than the minimum. I find the cumulative distribution plot very informative. It clearly shows that our data is highly skewed and we will have to take this into account as we divide our data into successive intervals (clusters). Note that one possible option to deal with the skewness could be to transform the data, but maps with nonlinear transformation of original counts are difficult to interpret. You might pursue this option if you are sharing your maps with an audience that is familiar with such transofrmations and is likely to understand it quickly. We will not follow this approach. 

So, one of our tasks now is to partition the highly skewed distribution into a small number of intervals. From our discussion it should be clear that we will map these groups to a sequential color scheme (from low to high). If you search the literature, you will find many different ways of partiotining the data. We will restrict ourselves to simple, intuitive approaches that are effective and are supported in software.   

The package 'classInt' has some useful functionalities. Read the help documentation for the function 'classIntervals', especially about the parameter 'style'. The RColorBrewer package (http://colorbrewer.org) provides color palettes for the three types of color schemes: divergent ('div'), qualitative ('qual'), and sequential('seq'). Please cite them if you end up using their package. 

I will choose 5 clusters for all the subsequent analysis, but you must remember that this is a  'free parameter' in your chloropleth map. Ideally you would like to "experiment" a bit and narrow down to something that strikes a balance between faithfully representing the information and condensing it in a few categories so that your reader can quickly grasp the message. 

We will try the partitions we discussed, but there are other options in the function classIntervals for partitioning the data such as 'hclust' (hierarchical clustering), 'kmeans', and 'equal'. 

```{r}
grQ = classIntervals(state.total$cases, n = 5, style = 'quantile')
grJ = classIntervals(state.total$cases, n=5, style = 'jenks')
grF = classIntervals(state.total$cases, n=5, style = 'fisher')

display.brewer.pal(5, 'BuGn')  
display.brewer.pal(5, 'Greys')
display.brewer.pal(5, 'YlGnBu')    # I prefer 1- or 2-colors for sequential

par(mfrow = c(1,2))
colPal1 = brewer.pal(5, 'BuGn')
plot(grJ, colPal1, main = 'Jenks Intervals', xlab = 'No of Cases', ylab = 'Relative Frequency')
colPal2 = brewer.pal(5, 'Greys')
plot(grQ, colPal2, main = 'Quantile Intervals', xlab = 'No of Cases', ylab = 'Relative Frequency')

```
Look at the color coding at the bottom of the plots and note how case counts are grouped. I am printing only two classifications, but you can use the code above to try the third one too. Do any of the three partitions we tried - 'quantile', 'jenks', or 'fisher' - satisfy you? Do these schemes help you summarize the data well and also faithfully represent the nuiances in the data?  

I do not think either of these provide a good scheme to discritize the number of cases. Purely data-driven partition might seem 'objective', but it in many situtations it may not be optimal unless your attribute data follows one of the idealized distribution or you happen to have plenty of data. On the other extreme, you can discritize case count compltely based on exogeneous categories such as less than 1000, 1000 to 5000, 5000 to 10,000 etc. In fact, it is common to see maps from news media websites that follow this approach. In my limited experience you have to spend a lot of time, and I emphasize, a lot of time, to get a partition that is objective, simple, and does the job.  

A lot of data and information can be productively organized into hierarchies. So, if I am not getting any other ideas to organize and partition a given data (or any information), I try a hierarchical approach. It seems to me that we can identify two or three higher level regimes (Do you agree?) and then perhaps further groups within some of them. If you want to follow this approach, try fitting linear splines and then further partition the data withon each interval demarcated by the knots.  

In addition to having a meaningful partition, there are other points that you should consider. *Ideally your discritization should not be very sensitive to small perturbations in the boundaries, so you should carefully look at the states that are close to partition boundaries. In particular, states that are adjacent to each other in space and are on the either side of the partition boundary in the discritized attribute space deserve close attention.* In the interest of time we will not pursue any of these, but you should try it later. To complete this part let us make a few maps and examine. 

We will try to lable each state with its two letters abbreviation. To do this we get centroid of each polygon and put text-label. This results in some clutter in the north eastern states with small area. You can nudge the coordinates a bit to separate the labels.  
```{r}
grBreaks = grQ[2]$brks
grBreaks[1] = grBreaks[1]-1
state.total['Group1'] = cut(state.total$cases, grBreaks, right=TRUE)
state.total['Group2'] = cut(state.total$cases, c(400, 1000, 5000, 10000, 40000, 400000), right=TRUE)
state.total = state.total[order(state.total$Group1),]
rownames(state.total) = as.character(1:dim(state.total)[1])
head(state.total)
tail(state.total)
# If you know mutate, try to use it on state.total instead of the code above

ggplot(data = state.total) + geom_sf(aes(fill=Group1)) + scale_fill_manual(values = colPal1) +labs(fill = 'Case Count')
label.vec = c('400-1000', '1000-5000', '5000-10000', '10000-40000', '40000-400000' )
ggplot(data = state.total) + geom_sf(aes(fill=Group2)) + scale_fill_manual(values = colPal1, labels=label.vec) +labs(fill = 'Case Count')

# Let us try to label each state - centroid needs projected coordinates
projString = "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs"

poly.cent =st_transform(state.total['state_abbr'], projString)
poly.cent = st_centroid(poly.cent)
poly.cent = st_transform(poly.cent, st_crs(state.total))  # Back to geographic coordinates
cent.coordinates = st_coordinates(poly.cent)
poly.cent = data.frame('state_abbr' = poly.cent$state_abbr, 'latitude' = cent.coordinates[,2], 'longitude' = cent.coordinates[,1])

# Join to state.total
state.total = left_join(state.total, poly.cent, by = 'state_abbr')
state.total = st_as_sf(state.total)

gt = ggplot(data = state.total) + geom_sf(aes(fill=Group2)) + scale_fill_manual(values = colPal1, labels=label.vec) +labs(fill = 'Case Count')
gt+ geom_text(aes(label = state_abbr, x = longitude, y = latitude), size=3) + xlab('Longitude') + ylab('Latitude')

 
```
You would perhaps like to see the parititon plot we created earlier and the map above together. You can try 'cowplot' or 'gridExtra'. If you want to format axis lables see this (https://stackoverflow.com/questions/11610377/how-do-i-change-the-formatting-of-numbers-on-an-axis-with-ggplot). You must be wondering that with web-based mapping we can easily provide more information. We will try that later, for now let us pursue the static map option a bit further. 

## 1.2. Mapping Case Ratio 
One of the reasons that case count has such a skewed distribution is the difference in population. Let us try and normalize count with state level population. We will use 2010 census population. Download the data, normalize case count with the population, then follow the steps above, and prepare a map with case count per 1000 persons. If you want, you can normalize it in the opposite direction by how many people per case. 

Just a reminder that you need a census API key (that you obatined from Census) in your R environment.
```{r, message=FALSE}
total.pop = get_decennial(geography = 'state', variables = 'P001001', year=2010, geometry = FALSE)
setnames(total.pop, 'value', 'Population')
total.pop = total.pop [c('GEOID', 'Population')]
state.total = left_join(state.total, total.pop, by = 'GEOID')
state.total ['CasePer1000'] = (state.total$cases / state.total$Population)*1000

#We will try the same partitions but on case ratio per thousand.   
grQ = classIntervals(state.total$CasePer1000, n = 5, style = 'quantile')
grJ = classIntervals(state.total$CasePer1000, n=5, style = 'fixed', fixedBreaks = c(0, 2, 5, 10, 15, 20))

par(mfrow = c(1,2))
colPal1 = brewer.pal(5, 'BuGn')
plot(grJ, colPal1, main = 'Exogeneous Intervals', xlab = 'Case per 1000', ylab = 'Relative Frequency')
colPal2 = brewer.pal(5, 'Greys')
plot(grQ, colPal2, main = 'Quantile Intervals', xlab = 'Case per 1000', ylab = 'Relative Frequency')
```
Now create the two maps again using cases per 1000 persons. For the first one use the Jenks method and for the other use the categories we made (what we called exogeneous). You would perhaps see a number of maps from news media websites following the exogeneous approach (https://www.theguardian.com/world/ng-interactive/2020/may/28/coronavirus-map-us-latest-cases-state-by-state). 

*Please note that you can get similar ratio with the very different numbers, in particular, [low/low i.e. low number of cases relative to small population] and [high/high i.e. high case count with large population] can give similar numbers. But, depending on the context they can have very different meaning.* 
We will not try to resolve this here but please refer to slides and our discussion. If you want to know more, the paper  (Cromley RG, Zhang S, Vorotyntseva N. A concentration-based approach to data classification for choropleth mapping. International Journal of Geographical Information Science. 2015;29(10):1845-63) in the workshop box-folder shows some ways to understand and resolve this.    

```{r}
grBreaks = grQ[2]$brks
grBreaks[1] = grBreaks[1]-0.001
state.total['Group3'] = cut(state.total$CasePer1000, grBreaks)

ggplot(data = state.total) + geom_sf(aes(fill=Group3)) + scale_fill_manual(values = colPal1) +labs(fill = 'Risk') + geom_text(aes(label = state_abbr, x = longitude, y = latitude))

state.total['Group4'] = cut(state.total$CasePer1000, c(0, 1, 5, 10, 15, 20), right=TRUE)
label.vec2 = c(' less than 1', '1-5', '5-10', '10-15', '15-20' )
ggplot(data = state.total) + geom_sf(aes(fill=Group4)) + scale_fill_manual(values = colPal1, labels=label.vec2) +labs(fill = 'Risk') + geom_text(aes(label = state_abbr, x = longitude, y = latitude), size=3) + xlab('Longitude') + ylab('Latitude')


```
A comment here about ggplot. There are a lot of changes that you can make here with ggplot's functionalities. Library has a book (ggplot2) written by Hadley Wickhem, the lead developer of ggplot. It is an easy read and is full of clever examples. If your regularly use ggplot, you should try to read Chapter 4. 

Sometimes you might see what is called an unclassed choropleth map, but it is very rarely a good approach. So, we will not be discussing it. You might have seen examples from news media sites where they compare state level rates with the national rates. So, you map a type of location quotient. If you want you can try this, it might be a good idea and can give you a very different information e.g. wich state are significantly higher than the national average and which ones are below. You may also have seen examples with hypothesis test on location quotient, but you have to be careful before you consider state level metrics as outcome of stationary stochastic processes.  

## 1.3. First leaflet map
R has bindings on both Leaflet (https://leafletjs.com/) and OpenLayers (https://openlayers.org/), the two very popular JavaScript libraries to create web maps. OpenLayers is perhaps much more powerful, but demands more investment to learn and figure things out. Leaflet is simple, easy to learn, and is likely to be adequate for a lot of your web mapping needs. 

Let us create a web-map with case ratio. We also want a pop-up that displays total no of cases (approximated to nearest tens) and population (approximated to nearest 100s). Pop-ups allow us to incorporate more information without cluttering the map. Leaflet syntax accepts dplyr-type piping. Note that Leaflet can take 'sf' objects but expects data in geographic coordinates so makes sure that your data is in WGS84 (EPSG code 4326).   

```{r}
bins = c(0, 1, 5, 10, 15, 20)
color.bin = colorBin("BuGn", domain = state.total$CasePer1000, bins = bins)

state.total = st_transform(state.total, 4326)

# 1. First attempt
leaflet(state.total) %>% 
  addTiles(group = "OSM (default)", options = tileOptions(opacity = 0.3))%>%
  addPolygons(fillColor = ~color.bin(CasePer1000), opacity = 1, weight=2, stroke = TRUE, color = '#FFF', fillOpacity = 1) %>% 
  addLegend(pal = color.bin, values = ~CasePer1000, opacity = 0.7, title = NULL,
            position = "bottomright") 
  
# 2. Try case ratio map with count as a pop up

# Prepare popup label - - if you know HTML there might be more elegent solutions (perhaps via htmltools).

df.forlabel = state.total[c('state_abbr', 'cases')]
st_geometry(df.forlabel) = NULL

# You have to coerce case count to be a numeric otherwise by default it shows as a character
popup.label = apply(as.matrix(df.forlabel), 1, function (x) sprintf("Total cases in %s: %d", x[1], round(as.integer(x[2]), digits = -1)))
state.total['Popup'] = popup.label

leaflet(state.total) %>% 
  addTiles(group = "OSM (default)", options = tileOptions(opacity = 0.3))%>%
  addPolygons(fillColor = ~color.bin(CasePer1000), 
              opacity = 1, weight=1, stroke = TRUE, 
              color = '#000', fillOpacity = 1,  
              highlight = highlightOptions(weight = 1, color = "#666",  bringToFront = TRUE),
              label = ~Popup) %>% 
  addLegend(pal = color.bin, values = ~CasePer1000, opacity = 1, title = 'Cases Per 1000',
            position = "topright")

# 3.3 Try case count
bins = c(400, 1000, 5000, 10000, 40000, 400000)  
color.bin = colorBin("BuGn", domain = state.total$cases, bins = bins)

df.forlabel = state.total[c('state_abbr', 'CasePer1000')]
st_geometry(df.forlabel) = NULL


# Prepare popup label 
popup.label = apply(as.matrix(df.forlabel), 1, function (x) sprintf("Case per 1000 in %s: %d", x[1], round(as.integer(x[2]), digits = 2)))
state.total['Popup'] = popup.label


leaflet(state.total) %>% 
  addTiles(group = "OSM (default)", options = tileOptions(opacity = 0.3))%>%
  addPolygons(fillColor = ~color.bin(cases), 
              opacity = 1, weight=1, stroke = TRUE, 
              color = '#000', fillOpacity = 1, label = ~Popup,  
              highlight = highlightOptions(weight = 2, color = "#666",  bringToFront = TRUE)) %>% 
  addLegend(pal = color.bin, values = ~cases, opacity = 0.7, title = 'Total Cases',
            position = "topright")

```
